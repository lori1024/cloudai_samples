{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The following code was used in a live demonstration at the Cloud AI Huddle held on June 14, 2018.\n",
    "In this notebook, we will train a click-through-rate prediction model using the Criteo dataset\n",
    "and then submit it to the Cloud for prediction. We will also package up the code and train\n",
    "the model in the cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this notebook, we'll use the [criteo dataset](http://labs.criteo.com/2014/09/kaggle-contest-dataset-now-available-academic-use/) and split it across multiple days, to simulate the use case of training a model daily on the previous 7 days worth of data.\n",
    "\n",
    "1. [Download the data](http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/) (you'll need to agree to the terms of service)\n",
    "2. Untar the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "readme.txt\n",
      "tar: Ignoring unknown extended header keyword 'LIBARCHIVE.creationtime'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "test.txt\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.dev'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.ino'\n",
      "tar: Ignoring unknown extended header keyword 'SCHILY.nlink'\n",
      "train.txt\n"
     ]
    }
   ],
   "source": [
    "! tar -xvf dac.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Split the data into 30 roughly equally sized chunks with each file named click-DD.txt. This simulates having 30 days worth of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "! split -d -n l/30 train.txt click- --additional-suffix=.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Move each file into its own sub-directory of `logs/`. We'll make today the last day of logs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "! mkdir logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date\n",
    "from datetime import timedelta\n",
    "\n",
    "lastday = date.today()\n",
    "for i in range(30):\n",
    "    # Move click-DD.txt to a subdirectory of logs with the format YYYYMMDD.\n",
    "    # We need to compute appropriate offset from lastday: -00 becomes 30 days in the past.\n",
    "    offset = 30 - i\n",
    "    daystamp = (lastday - timedelta(offset)).strftime('%Y%m%d')\n",
    "    subdir = os.path.join('logs', daystamp)\n",
    "    os.mkdir(subdir)\n",
    "    os.rename('click-{:02d}.txt'.format(i), os.path.join(subdir, 'click.txt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a CTR Model\n",
    "\n",
    "We will train a click-through rate predictor on the previous 7 days worth of data. Note that in\n",
    "order to avoid using large amounts of RAM (I'm training this on a Chromebook), we limit \n",
    "MAX_SAMPLES to a small number. However, one advantage of the cloud is the ability to use\n",
    "machines with lots of RAM and we can get rid of the `MAX_SAMPLES` if we want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/dist-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./logs/20180610/click.txt\n",
      "Reading file ./logs/20180611/click.txt\n",
      "Reading file ./logs/20180612/click.txt\n",
      "Reading file ./logs/20180613/click.txt\n",
      "Reading file ./logs/20180614/click.txt\n",
      "Reading file ./logs/20180615/click.txt\n",
      "Reading file ./logs/20180616/click.txt\n",
      "training data size: 56000\n",
      "test data size: 14000\n",
      "Training...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Python script to train sklearn model using the criteo dataset.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from datetime import timedelta\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "import xgboost\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--base-dir', default='.')\n",
    "parser.add_argument('--event-date',\n",
    "                    default=(date.today() - timedelta(1)).strftime('%Y%m%d'))\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "# Number of data samples to use for training.\n",
    "MAX_SAMPLES = 70000\n",
    "\n",
    "\n",
    "# Read in 7 days worth of data (starting yesterday) into a key value format.\n",
    "\n",
    "# NB: end_date is exclusive\n",
    "def daterange(end_date, num_days):\n",
    "    for n in range(num_days):\n",
    "        yield end_date - timedelta(num_days - n)\n",
    "\n",
    "# Data in key value format. Each element is a dict.\n",
    "data = []\n",
    "\n",
    "# Labels for the data.\n",
    "y = []\n",
    "\n",
    "num_samples = 0\n",
    "for d in daterange(datetime.strptime(args.event_date, '%Y%m%d'), 7):\n",
    "  data_fn = os.path.join(args.base_dir, 'logs', d.strftime('%Y%m%d'), 'click.txt')\n",
    "  print('Reading file {}'.format(data_fn))\n",
    "  with file_io.FileIO(data_fn, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    while num_samples < MAX_SAMPLES:\n",
    "      line = reader.next()\n",
    "      row = {}\n",
    "      y.append(int(line[0]))\n",
    "      for i in range(1, 13 + 1):\n",
    "        if line[i]:\n",
    "          row[str(i)] = int(line[i])\n",
    "      for i in range(14, 39 + 1):\n",
    "        if line[i]:\n",
    "          row[str(i)] = line[i]\n",
    "      data.append(row)\n",
    "      num_samples += 1\n",
    "\n",
    "# Split data into training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2)\n",
    "print 'training data size: {}'.format(len(X_train))\n",
    "print 'test data size: {}'.format(len(X_test))\n",
    "\n",
    "param = {\n",
    "    'max_depth': 7,\n",
    "    'eta': 0.2,\n",
    "    'silent': False,\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'logloss',\n",
    "    'gamma': 0.4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 20,\n",
    "    'alpha': 3,\n",
    "    'lambda': 100\n",
    "}\n",
    "xgb = xgboost.XGBRegressor(n_estimators=2, **param)\n",
    "\n",
    "# Setup and train the pipeline.\n",
    "pipeline = Pipeline(steps=[(\"dict_vect\", DictVectorizer()),\n",
    "                           (\"xgboost\", xgb)])\n",
    "print(\"Training...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Locally\n",
    "\n",
    "Let's use our pipeline to produce a prediction (locally/in memory)!\n",
    "\n",
    "The input (`example`) here is not the most traditional format, but it has most of the properties\n",
    "we want: it is sparse, i.e. allows for missing values, does not require us to manually one-hot\n",
    "encode the categorical variables, etc. The advantage of this format is that it very closely\n",
    "matches the JSON input we will be sending to the cloud.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.4866918]\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    \"1\": 26,\n",
    "    \"2\": 5,\n",
    "    \"3\": 7,\n",
    "    \"4\": 7,\n",
    "    \"5\": 250,\n",
    "    \"6\": 7,\n",
    "    \"7\": 161,\n",
    "    \"8\": 45,\n",
    "    \"9\": 965,\n",
    "    \"10\": 2,\n",
    "    \"11\": 30,\n",
    "    \"13\": 7,\n",
    "    \"14\": \"5a9ed9b0\",\n",
    "    \"15\": \"421b43cd\",\n",
    "    \"16\": \"8d7d4f0a\",\n",
    "    \"17\": \"29998ed1\",\n",
    "    \"18\": \"25c83c98\",\n",
    "    \"19\": \"fbad5c96\",\n",
    "    \"20\": \"db55c967\",\n",
    "    \"21\": \"0b153874\",\n",
    "    \"22\": \"a73ee510\",\n",
    "    \"23\": \"3b08e48b\",\n",
    "    \"24\": \"4ce044d9\",\n",
    "    \"25\": \"6aaba33c\",\n",
    "    \"26\": \"a4fb9828\",\n",
    "    \"27\": \"b28479f6\",\n",
    "    \"28\": \"e1ac77f7\",\n",
    "    \"29\": \"b041b04a\",\n",
    "    \"30\": \"e5ba7672\",\n",
    "    \"31\": \"2804effd\",\n",
    "    \"34\": \"723b4dfd\",\n",
    "    \"36\": \"bcdee96c\",\n",
    "    \"37\": \"b34f3128\",\n",
    "}\n",
    "predictions = pipeline.predict([example])\n",
    "print(\"Prediction: {}\".format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Setup\n",
    "\n",
    "Running in the cloud inherently requires a bit more setup such as creating an account in the\n",
    "cloud, enabling the Cloud ML Engine API, etc.\n",
    "\n",
    "Please follow the instructions [here](https://github.com/GoogleCloudPlatform/cloudml-samples/blob/master/xgboost/notebooks/Online%20Prediction%20with%20XGBoost.ipynb).\n",
    "\n",
    "Once that is all setup, let's set a few convenient environment variables.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=op-beta-walkthrough\n",
      "env: MODEL_DEST=gs://op-beta-walkthrough-mlengine/ctr\n",
      "env: MODEL_NAME=ctr\n",
      "env: VERSION_NAME=v2\n",
      "env: REGION=us-central1\n"
     ]
    }
   ],
   "source": [
    "% env PROJECT_ID op-beta-walkthrough\n",
    "% env MODEL_DEST gs://op-beta-walkthrough-mlengine/ctr\n",
    "% env MODEL_NAME ctr\n",
    "% env VERSION_NAME v2\n",
    "% env REGION us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save and Upload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model.joblib']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the model.\n",
    "model = './model.joblib'\n",
    "joblib.dump(pipeline, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "-\n",
      "Operation completed over 1 objects/6.1 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp model.joblib ${MODEL_DEST}/model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy to Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/op-beta-walkthrough/models/ctr].\n"
     ]
    }
   ],
   "source": [
    "! gcloud ml-engine models create $MODEL_NAME --enable-logging --regions us-central1 --project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"projects/op-beta-walkthrough/operations/create_ctr_v2-1529073392835\",\n",
      "  \"metadata\": {\n",
      "    \"@type\": \"type.googleapis.com/google.cloud.ml.v1.OperationMetadata\",\n",
      "    \"createTime\": \"2018-06-15T14:36:33Z\",\n",
      "    \"operationType\": \"CREATE_VERSION\",\n",
      "    \"modelName\": \"projects/op-beta-walkthrough/models/ctr\",\n",
      "    \"version\": {\n",
      "      \"name\": \"projects/op-beta-walkthrough/models/ctr/versions/v2\",\n",
      "      \"deploymentUri\": \"gs://op-beta-walkthrough-mlengine/ctr\",\n",
      "      \"createTime\": \"2018-06-15T14:36:32Z\",\n",
      "      \"runtimeVersion\": \"1.8\",\n",
      "      \"framework\": \"SCIKIT_LEARN\",\n",
      "      \"pythonVersion\": \"2.7\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl -X POST -H \"Content-Type: application/json\" \\\n",
    "   -d '{\"name\": \"'${VERSION_NAME}'\", \"deploymentUri\": \"'${MODEL_DEST}'\", \"runtimeVersion\": \"1.8\", \"framework\": \"SCIKIT_LEARN\"}' \\\n",
    "   -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    https://ml.googleapis.com/v1/projects/$PROJECT_ID/models/$MODEL_NAME/versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud: 0.522804915905\n",
      "Local: 0.48669180274\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "VERSION_NAME = os.environ['VERSION_NAME']\n",
    "MODEL_NAME = os.environ['MODEL_NAME']\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)\n",
    "name += '/versions/{}'.format(VERSION_NAME)\n",
    "\n",
    "response = service.projects().predict(\n",
    "    name=name,\n",
    "    body={'instances': [example]}\n",
    ").execute()\n",
    "\n",
    "cloud_prediction = response['predictions'][0]\n",
    "\n",
    "# Compare cloud prediction to local prediction\n",
    "# use the same `example` from the \"Predict locally\" section\n",
    "local_prediction = pipeline.predict([example])[0]\n",
    "print(\"Cloud: {}\\nLocal: {}\".format(cloud_prediction, local_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Cloud\n",
    "\n",
    "As discussed in the presentation, there are times when training in the cloud is very\n",
    "convenient. For example, you may need lots of RAM (up to 4 TBs!) to load full datasets in\n",
    "memory or lots of cores to do hyperparameter tuning.\n",
    "\n",
    "You can also use this same code to create a pipeline, e.g., using Airflow (a managed version\n",
    "of which is available as Cloud Composer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=gs://ml-engine-airflow/criteo\n",
      "env: TRAIN_BIN=gs://ml-engine-airflow/criteo/bin/criteo-trainer-0.1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "% env BASE_DIR=gs://ml-engine-airflow/criteo\n",
    "% env TRAIN_BIN=gs://ml-engine-airflow/criteo/bin/criteo-trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing criteo_trainer.egg-info/PKG-INFO\n",
      "writing top-level names to criteo_trainer.egg-info/top_level.txt\n",
      "writing dependency_links to criteo_trainer.egg-info/dependency_links.txt\n",
      "reading manifest file 'criteo_trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'criteo_trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating criteo-trainer-0.1\n",
      "creating criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "creating criteo-trainer-0.1/trainer\n",
      "copying files to criteo-trainer-0.1...\n",
      "copying setup.py -> criteo-trainer-0.1\n",
      "copying criteo_trainer.egg-info/PKG-INFO -> criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "copying criteo_trainer.egg-info/SOURCES.txt -> criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "copying criteo_trainer.egg-info/dependency_links.txt -> criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "copying criteo_trainer.egg-info/top_level.txt -> criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "copying trainer/__init__.py -> criteo-trainer-0.1/trainer\n",
      "copying trainer/train.py -> criteo-trainer-0.1/trainer\n",
      "Writing criteo-trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'criteo-trainer-0.1' (and everything under it)\n",
      "Copying file://dist/criteo-trainer-0.1.tar.gz...\n",
      "/ [1 files][  2.0 KiB/  2.0 KiB]                                                \n",
      "Operation completed over 1 objects/2.0 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! python setup.py sdist\n",
    "! gsutil cp dist/criteo-trainer-0.1.tar.gz ${TRAIN_BIN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [ctr_1529013637] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe ctr_1529013637\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs ctr_1529013637\n",
      "jobId: ctr_1529013637\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "! gcloud ml-engine jobs submit training ctr_`date +%s` \\\n",
    "    --module-name=trainer.train \\\n",
    "    --packages=${TRAIN_BIN} \\\n",
    "    --runtime-version=1.8 \\\n",
    "    --region=us-central1 \\\n",
    "    --project=ml-engine-airflow \\\n",
    "    -- \\\n",
    "      --base-dir=${BASE_DIR} \\\n",
    "      --event-date=20180608"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup a pipeline to train and push a new model on a daily basis\n",
    "\n",
    "The `airflow` sub-directory includes an Airflow DAG, `ctr_daily_pipeline.py`, which can be used to create a pipeline that trains on a daily basis. It is outside the scope of this notebook file to explain how to set it up.\n",
    "\n",
    "Until Cloud Composer supports Airflow v1.10, you will additionally need the `mlengine_operator.py` files, also found in the `airflow` sub-directory."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
