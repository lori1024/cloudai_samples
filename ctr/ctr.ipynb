{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a CTR Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading file ./logs/20180606/click.txt\n",
      "Reading file ./logs/20180607/click.txt\n",
      "Reading file ./logs/20180608/click.txt\n",
      "Reading file ./logs/20180609/click.txt\n",
      "Reading file ./logs/20180610/click.txt\n",
      "Reading file ./logs/20180611/click.txt\n",
      "Reading file ./logs/20180612/click.txt\n",
      "training data size: 56000\n",
      "test data size: 14000\n",
      "Training...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"Python script to train sklearn model using the criteo dataset.\"\"\"\n",
    "\n",
    "import argparse\n",
    "import csv\n",
    "from datetime import date\n",
    "from datetime import datetime\n",
    "from datetime import time\n",
    "from datetime import timedelta\n",
    "import subprocess\n",
    "import os\n",
    "\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "from tensorflow.python.lib.io import file_io\n",
    "\n",
    "import xgboost\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--base-dir', default='.')\n",
    "parser.add_argument('--event-date',\n",
    "                    default=(date.today() - timedelta(1)).strftime('%Y%m%d'))\n",
    "args = parser.parse_args([])\n",
    "\n",
    "\n",
    "# Dataset: http://labs.criteo.com/2014/02/kaggle-display-advertising-challenge-dataset/\n",
    "\n",
    "# Number of data samples to use for training.\n",
    "MAX_SAMPLES = 70000\n",
    "\n",
    "\n",
    "# Read in 7 days worth of data (starting yesterday) into a key value format.\n",
    "\n",
    "# NB: end_date is exclusive\n",
    "def daterange(end_date, num_days):\n",
    "    for n in range(num_days):\n",
    "        yield end_date - timedelta(num_days - n)\n",
    "\n",
    "# Data in key value format. Each element is a dict.\n",
    "data = []\n",
    "\n",
    "# Labels for the data.\n",
    "y = []\n",
    "\n",
    "num_samples = 0\n",
    "for d in daterange(datetime.strptime(args.event_date, '%Y%m%d'), 7):\n",
    "  data_fn = os.path.join(args.base_dir, 'logs', d.strftime('%Y%m%d'), 'click.txt')\n",
    "  print('Reading file {}'.format(data_fn))\n",
    "  with file_io.FileIO(data_fn, 'r') as f:\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    while num_samples < MAX_SAMPLES:\n",
    "      line = reader.next()\n",
    "      row = {}\n",
    "      y.append(int(line[0]))\n",
    "      for i in range(1, 13 + 1):\n",
    "        if line[i]:\n",
    "          row[str(i)] = int(line[i])\n",
    "      for i in range(14, 39 + 1):\n",
    "        if line[i]:\n",
    "          row[str(i)] = line[i]\n",
    "      data.append(row)\n",
    "      num_samples += 1\n",
    "\n",
    "# Split data into training and testing.\n",
    "X_train, X_test, y_train, y_test = train_test_split(data, y, test_size=0.2)\n",
    "print 'training data size: {}'.format(len(X_train))\n",
    "print 'test data size: {}'.format(len(X_test))\n",
    "\n",
    "param = {\n",
    "    'max_depth': 7,\n",
    "    'eta': 0.2,\n",
    "    'silent': False,\n",
    "    'objective': 'binary:logistic',\n",
    "    'tree_method': 'hist',\n",
    "    'eval_metric': 'logloss',\n",
    "    'gamma': 0.4,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'min_child_weight': 20,\n",
    "    'alpha': 3,\n",
    "    'lambda': 100\n",
    "}\n",
    "xgb = xgboost.XGBRegressor(n_estimators=2, **param)\n",
    "\n",
    "# Setup and train the pipeline.\n",
    "pipeline = Pipeline(steps=[(\"dict_vect\", DictVectorizer()),\n",
    "                           (\"xgboost\", xgb)])\n",
    "print(\"Training...\")\n",
    "pipeline.fit(X_train, y_train)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction: [0.4866918]\n"
     ]
    }
   ],
   "source": [
    "example = {\n",
    "    \"1\": 26,\n",
    "    \"2\": 5,\n",
    "    \"3\": 7,\n",
    "    \"4\": 7,\n",
    "    \"5\": 250,\n",
    "    \"6\": 7,\n",
    "    \"7\": 161,\n",
    "    \"8\": 45,\n",
    "    \"9\": 965,\n",
    "    \"10\": 2,\n",
    "    \"11\": 30,\n",
    "    \"13\": 7,\n",
    "    \"14\": \"5a9ed9b0\",\n",
    "    \"15\": \"421b43cd\",\n",
    "    \"16\": \"8d7d4f0a\",\n",
    "    \"17\": \"29998ed1\",\n",
    "    \"18\": \"25c83c98\",\n",
    "    \"19\": \"fbad5c96\",\n",
    "    \"20\": \"db55c967\",\n",
    "    \"21\": \"0b153874\",\n",
    "    \"22\": \"a73ee510\",\n",
    "    \"23\": \"3b08e48b\",\n",
    "    \"24\": \"4ce044d9\",\n",
    "    \"25\": \"6aaba33c\",\n",
    "    \"26\": \"a4fb9828\",\n",
    "    \"27\": \"b28479f6\",\n",
    "    \"28\": \"e1ac77f7\",\n",
    "    \"29\": \"b041b04a\",\n",
    "    \"30\": \"e5ba7672\",\n",
    "    \"31\": \"2804effd\",\n",
    "    \"34\": \"723b4dfd\",\n",
    "    \"36\": \"bcdee96c\",\n",
    "    \"37\": \"b34f3128\",\n",
    "}\n",
    "predictions = pipeline.predict([example])\n",
    "print(\"Prediction: {}\".format(predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cloud Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: PROJECT_ID=op-beta-walkthrough\n",
      "env: MODEL_DEST=gs://op-beta-walkthrough-mlengine/ctr\n",
      "env: MODEL_NAME=ctr\n",
      "env: VERSION_NAME=v2\n",
      "env: REGION=us-central1\n"
     ]
    }
   ],
   "source": [
    "% env PROJECT_ID op-beta-walkthrough\n",
    "% env MODEL_DEST gs://op-beta-walkthrough-mlengine/ctr\n",
    "% env MODEL_NAME ctr\n",
    "% env VERSION_NAME v2\n",
    "% env REGION us-central1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save and Upload the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./model.joblib']"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Export the model.\n",
    "model = './model.joblib'\n",
    "joblib.dump(pipeline, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying file://model.joblib [Content-Type=application/octet-stream]...\n",
      "-\n",
      "Operation completed over 1 objects/6.1 MiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! gsutil cp model.joblib ${MODEL_DEST}/model.joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy to Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created ml engine model [projects/op-beta-walkthrough/models/ctr].\n"
     ]
    }
   ],
   "source": [
    "! gcloud ml-engine models create $MODEL_NAME --enable-logging --regions us-central1 --project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"name\": \"projects/op-beta-walkthrough/operations/create_ctr_v2-1529073392835\",\n",
      "  \"metadata\": {\n",
      "    \"@type\": \"type.googleapis.com/google.cloud.ml.v1.OperationMetadata\",\n",
      "    \"createTime\": \"2018-06-15T14:36:33Z\",\n",
      "    \"operationType\": \"CREATE_VERSION\",\n",
      "    \"modelName\": \"projects/op-beta-walkthrough/models/ctr\",\n",
      "    \"version\": {\n",
      "      \"name\": \"projects/op-beta-walkthrough/models/ctr/versions/v2\",\n",
      "      \"deploymentUri\": \"gs://op-beta-walkthrough-mlengine/ctr\",\n",
      "      \"createTime\": \"2018-06-15T14:36:32Z\",\n",
      "      \"runtimeVersion\": \"1.8\",\n",
      "      \"framework\": \"SCIKIT_LEARN\",\n",
      "      \"pythonVersion\": \"2.7\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "! curl -X POST -H \"Content-Type: application/json\" \\\n",
    "   -d '{\"name\": \"'${VERSION_NAME}'\", \"deploymentUri\": \"'${MODEL_DEST}'\", \"runtimeVersion\": \"1.8\", \"framework\": \"SCIKIT_LEARN\"}' \\\n",
    "   -H \"Authorization: Bearer `gcloud auth print-access-token`\" \\\n",
    "    https://ml.googleapis.com/v1/projects/$PROJECT_ID/models/$MODEL_NAME/versions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict on Cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cloud: 0.522804915905\n",
      "Local: 0.48669180274\n"
     ]
    }
   ],
   "source": [
    "import googleapiclient.discovery\n",
    "\n",
    "PROJECT_ID = os.environ['PROJECT_ID']\n",
    "VERSION_NAME = os.environ['VERSION_NAME']\n",
    "MODEL_NAME = os.environ['MODEL_NAME']\n",
    "\n",
    "service = googleapiclient.discovery.build('ml', 'v1')\n",
    "name = 'projects/{}/models/{}'.format(PROJECT_ID, MODEL_NAME)\n",
    "name += '/versions/{}'.format(VERSION_NAME)\n",
    "\n",
    "response = service.projects().predict(\n",
    "    name=name,\n",
    "    body={'instances': [example]}\n",
    ").execute()\n",
    "\n",
    "cloud_prediction = response['predictions'][0]\n",
    "\n",
    "# Compare cloud prediction to local prediction\n",
    "# use the same `example` from the \"Predict locally\" section\n",
    "local_prediction = pipeline.predict([example])[0]\n",
    "print(\"Cloud: {}\\nLocal: {}\".format(cloud_prediction, local_prediction))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on Cloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Package up the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: BASE_DIR=gs://ml-engine-airflow/criteo\n",
      "env: TRAIN_BIN=gs://ml-engine-airflow/criteo/bin/criteo-trainer-0.1.tar.gz\n"
     ]
    }
   ],
   "source": [
    "% env BASE_DIR=gs://ml-engine-airflow/criteo\n",
    "% env TRAIN_BIN=gs://ml-engine-airflow/criteo/bin/criteo-trainer-0.1.tar.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running sdist\n",
      "running egg_info\n",
      "writing criteo_trainer.egg-info/PKG-INFO\n",
      "writing top-level names to criteo_trainer.egg-info/top_level.txt\n",
      "writing dependency_links to criteo_trainer.egg-info/dependency_links.txt\n",
      "reading manifest file 'criteo_trainer.egg-info/SOURCES.txt'\n",
      "writing manifest file 'criteo_trainer.egg-info/SOURCES.txt'\n",
      "warning: sdist: standard file not found: should have one of README, README.rst, README.txt, README.md\n",
      "\n",
      "running check\n",
      "warning: check: missing required meta-data: url\n",
      "\n",
      "warning: check: missing meta-data: either (author and author_email) or (maintainer and maintainer_email) must be supplied\n",
      "\n",
      "creating criteo-trainer-0.1\n",
      "creating criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "creating criteo-trainer-0.1/trainer\n",
      "copying files to criteo-trainer-0.1...\n",
      "copying setup.py -> criteo-trainer-0.1\n",
      "copying criteo_trainer.egg-info/PKG-INFO -> criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "copying criteo_trainer.egg-info/SOURCES.txt -> criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "copying criteo_trainer.egg-info/dependency_links.txt -> criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "copying criteo_trainer.egg-info/top_level.txt -> criteo-trainer-0.1/criteo_trainer.egg-info\n",
      "copying trainer/__init__.py -> criteo-trainer-0.1/trainer\n",
      "copying trainer/train.py -> criteo-trainer-0.1/trainer\n",
      "Writing criteo-trainer-0.1/setup.cfg\n",
      "Creating tar archive\n",
      "removing 'criteo-trainer-0.1' (and everything under it)\n",
      "Copying file://dist/criteo-trainer-0.1.tar.gz...\n",
      "/ [1 files][  2.0 KiB/  2.0 KiB]                                                \n",
      "Operation completed over 1 objects/2.0 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! python setup.py sdist\n",
    "! gsutil cp dist/criteo-trainer-0.1.tar.gz ${TRAIN_BIN}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submit a training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job [ctr_1529013637] submitted successfully.\n",
      "Your job is still active. You may view the status of your job with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs describe ctr_1529013637\n",
      "\n",
      "or continue streaming the logs with the command\n",
      "\n",
      "  $ gcloud ml-engine jobs stream-logs ctr_1529013637\n",
      "jobId: ctr_1529013637\n",
      "state: QUEUED\n"
     ]
    }
   ],
   "source": [
    "! gcloud ml-engine jobs submit training ctr_`date +%s` \\\n",
    "    --module-name=trainer.train \\\n",
    "    --packages=${TRAIN_BIN} \\\n",
    "    --runtime-version=1.8 \\\n",
    "    --region=us-central1 \\\n",
    "    --project=ml-engine-airflow \\\n",
    "    -- \\\n",
    "      --base-dir=${BASE_DIR} \\\n",
    "      --event-date=20180608"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
